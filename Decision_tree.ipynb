{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file.\n",
    "data = pd.read_csv(\"task_1.csv\", skiprows=1)\n",
    "\n",
    "# Select the relevant numerical columns.\n",
    "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
    "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
    "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
    "data = data[selected_cols].dropna()\n",
    "\n",
    "# Shuffle the dataset.\n",
    "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "X = data_shuffled.drop('NSP', axis=1)\n",
    "\n",
    "# Map the diagnosis code to a human-readable label.\n",
    "def to_label(y):\n",
    "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
    "\n",
    "Y = data_shuffled['NSP'].apply(to_label)\n",
    "\n",
    "# Partition the data into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2126, 22)\n",
      "      LB   AC   FM   UC   DL   DS   DP  ASTV  MSTV  ALTV  ...   Min    Max  \\\n",
      "0  120.0  0.0  0.0  0.0  0.0  0.0  0.0  73.0   0.5  43.0  ...  62.0  126.0   \n",
      "1  132.0  4.0  0.0  4.0  2.0  0.0  0.0  17.0   2.1   0.0  ...  68.0  198.0   \n",
      "2  133.0  2.0  0.0  5.0  2.0  0.0  0.0  16.0   2.1   0.0  ...  68.0  198.0   \n",
      "3  134.0  2.0  0.0  6.0  2.0  0.0  0.0  16.0   2.4   0.0  ...  53.0  170.0   \n",
      "4  132.0  4.0  0.0  5.0  0.0  0.0  0.0  16.0   2.4   0.0  ...  53.0  170.0   \n",
      "\n",
      "   Nmax  Nzeros   Mode   Mean  Median  Variance  Tendency  NSP  \n",
      "0   2.0     0.0  120.0  137.0   121.0      73.0       1.0  2.0  \n",
      "1   6.0     1.0  141.0  136.0   140.0      12.0       0.0  1.0  \n",
      "2   5.0     1.0  141.0  135.0   138.0      13.0       0.0  1.0  \n",
      "3  11.0     0.0  137.0  134.0   137.0      13.0       1.0  1.0  \n",
      "4   9.0     0.0  137.0  136.0   138.0      11.0       1.0  1.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for DummyClassifier:\n",
      "0.7805882352941176\n"
     ]
    }
   ],
   "source": [
    "# Dummy clf\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for DummyClassifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for Decisin tree classifier:\n",
      "0.9270588235294117\n"
     ]
    }
   ],
   "source": [
    "# Decision tree clf\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for Decisin tree classifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for Random forest classifier:\n",
      "0.9405882352941177\n"
     ]
    }
   ],
   "source": [
    "# Random forest clf\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for Random forest classifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for Gradient boosting classifier:\n",
      "0.9488235294117647\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting clf\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for Gradient boosting classifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\96noh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\96noh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for Logistic regression classifier:\n",
      "0.8911764705882353\n"
     ]
    }
   ],
   "source": [
    "# Logistics regression clf\n",
    "clf = LogisticRegression(max_iter = 20000)\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for Logistic regression classifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for MLP classifier:\n",
      "0.8805882352941176\n"
     ]
    }
   ],
   "source": [
    "# MLP clf\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for MLP classifier:\n",
    "{np.mean(cross_val_score(clf, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting classifier performed the best on the default settings. Therefore we will tune the hyperparameter settings for this classifier. It would be better to tune the hyperparameters for each single model before deciding which model to use, but this would be very computationally heavy for our computers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\"learning_rate\":[0.05,0.1,0.2],\n",
    "    \"max_depth\":[3,5,7],\n",
    "    \"n_estimators\":[100,200,300]}]\n",
    "\n",
    "clf = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid=params, scoring = \"accuracy\", cv = 5)\n",
    "\n",
    "clf_fit = clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for GradientBoosting classifier: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "print(f\"Best params for GradientBoosting classifier: {clf_fit.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test data for our model: 0.9225352112676056\n"
     ]
    }
   ],
   "source": [
    "# Run the model on test data with the best parameters\n",
    "clf = GradientBoostingClassifier(learning_rate = 0.2, max_depth = 3, n_estimators = 100)\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "Yguess = clf.predict(Xtest)\n",
    "print(f\"Final accuracy on test data for our model: {accuracy_score(Ytest, Yguess)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the Gradientboost classifier and received an accuracy of approximately 92% on test data. We tuned the hyperparameter max_depth, learning rate and n_estimators. There are more hyperparameters you could tune but we decided these were enough for now. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        val_str = f'{self.value:.4g}' if isinstance(self.value, float) else str(self.value)\n",
    "        graph.node(node_id, val_str, style='filled')\n",
    "        return node_counter+1, node_id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter+1, node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(ABC, BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        Y = np.array(Y)        \n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "    \n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)        \n",
    "\n",
    "    # This is the recursive training \n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be the most common value in Y.\n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "    \n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:,feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "    \n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "    \n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return 1-sum(p**2 for p in ps)\n",
    "    \n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'maj_sum':\n",
    "            self.criterion_function = majority_sum_scorer\n",
    "        elif self.criterion == 'info_gain':\n",
    "            self.criterion_function = info_gain_scorer\n",
    "        elif self.criterion == 'gini':\n",
    "            self.criterion_function = gini_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Counter(Y)\n",
    "        return self.class_distribution.most_common(1)[0][0]\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return len(self.class_distribution) == 1\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])        \n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "\n",
    "        # The frequency tables corresponding to the parts *before and including*\n",
    "        # and *after* the current element.\n",
    "        low_distr = Counter()\n",
    "        high_distr = Counter(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "            \n",
    "            # Update the frequency tables.\n",
    "            low_distr[y_i] += 1\n",
    "            high_distr[y_i] -= 1\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i+1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(X_sorted[max_i] + X_sorted[max_i+1])\n",
    "        return max_score, feature, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy from 5-fold CV for depth=2 classifier:\n",
      "0.8911764705882353\n",
      "Mean accuracy from 5-fold CV for depth=3 classifier:\n",
      "0.9023529411764706\n",
      "Mean accuracy from 5-fold CV for depth=4 classifier:\n",
      "0.9105882352941176\n",
      "Mean accuracy from 5-fold CV for depth=6 classifier:\n",
      "0.9099999999999999\n",
      "Mean accuracy from 5-fold CV for depth=8 classifier:\n",
      "0.9099999999999999\n",
      "Mean accuracy from 5-fold CV for depth=10 classifier:\n",
      "0.9099999999999999\n"
     ]
    }
   ],
   "source": [
    "# Depth 2\n",
    "cls_2 = TreeClassifier(max_depth=2)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=2 classifier:\n",
    "{np.mean(cross_val_score(cls_2, Xtrain, Ytrain))}\"\"\")\n",
    "\n",
    "# Depth 3\n",
    "cls_3 = TreeClassifier(max_depth=3)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=3 classifier:\n",
    "{np.mean(cross_val_score(cls_3, Xtrain, Ytrain))}\"\"\")\n",
    "\n",
    "# Depth 4\n",
    "cls_4 = TreeClassifier(max_depth=4)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=4 classifier:\n",
    "{np.mean(cross_val_score(cls_4, Xtrain, Ytrain))}\"\"\")\n",
    "\n",
    "# Depth 6\n",
    "cls_6 = TreeClassifier(max_depth=6)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=6 classifier:\n",
    "{np.mean(cross_val_score(cls_6, Xtrain, Ytrain))}\"\"\")\n",
    "\n",
    "# Depth 8\n",
    "cls_8 = TreeClassifier(max_depth=8)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=8 classifier:\n",
    "{np.mean(cross_val_score(cls_8, Xtrain, Ytrain))}\"\"\")\n",
    "\n",
    "# Depth 10\n",
    "cls_10 = TreeClassifier(max_depth=10)\n",
    "# 5-fold (default) Cross-validation\n",
    "print(f\"\"\"Mean accuracy from 5-fold CV for depth=10 classifier:\n",
    "{np.mean(cross_val_score(cls_10, Xtrain, Ytrain))}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the Mean accuracy score above that the best performing model had depth=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test data for tree model, max_depth=4: 0.8896713615023474\n"
     ]
    }
   ],
   "source": [
    "# Run on test data\n",
    "final_cls = TreeClassifier(max_depth = 4)\n",
    "final_cls.fit(Xtrain, Ytrain)\n",
    "# Predict\n",
    "Yguess = final_cls.predict(Xtest)\n",
    "\n",
    "print(f\"Final accuracy on test data for tree model, max_depth=4: {accuracy_score(Ytest, Yguess)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"1163pt\" height=\"392pt\"\r\n",
       " viewBox=\"0.00 0.00 1163.09 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 1159.09,-388 1159.09,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"35.7468\" cy=\"-18\" rx=\"35.9954\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"35.7468\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normal</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"127.747\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"127.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M110.747,-123C110.747,-123 52.7468,-123 52.7468,-123 46.7468,-123 40.7468,-117 40.7468,-111 40.7468,-111 40.7468,-99 40.7468,-99 40.7468,-93 46.7468,-87 52.7468,-87 52.7468,-87 110.747,-87 110.747,-87 116.747,-87 122.747,-93 122.747,-99 122.747,-99 122.747,-111 122.747,-111 122.747,-117 116.747,-123 110.747,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"81.7468\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ALTV &gt; 7?</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.438,-86.799C65.8683,-74.6593 56.9554,-58.1897 49.5593,-44.5231\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.5664,-42.7259 44.7287,-35.597 46.4101,-46.0575 52.5664,-42.7259\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"76.7468\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.0556,-86.799C97.6253,-74.6593 106.538,-58.1897 113.934,-44.5231\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.084,-46.0575 118.765,-35.597 110.927,-42.7259 117.084,-46.0575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"189.747\" cy=\"-105\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"189.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M227.247,-210C227.247,-210 152.247,-210 152.247,-210 146.247,-210 140.247,-204 140.247,-198 140.247,-198 140.247,-186 140.247,-186 140.247,-180 146.247,-174 152.247,-174 152.247,-174 227.247,-174 227.247,-174 233.247,-174 239.247,-180 239.247,-186 239.247,-186 239.247,-198 239.247,-198 239.247,-204 233.247,-210 227.247,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"189.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ASTV &gt; 79.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>4&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.891,-173.799C151.789,-161.126 129.691,-143.734 111.908,-129.738\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113.593,-126.61 103.57,-123.175 109.263,-132.11 113.593,-126.61\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"157.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;3 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>4&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.747,-173.799C189.747,-162.163 189.747,-146.548 189.747,-133.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.247,-133.175 189.747,-123.175 186.247,-133.175 193.247,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"203.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"291.747\" cy=\"-105\" rx=\"35.9954\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"291.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normal</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"393.747\" cy=\"-105\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"393.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M418.747,-210C418.747,-210 326.747,-210 326.747,-210 320.747,-210 314.747,-204 314.747,-198 314.747,-198 314.747,-186 314.747,-186 314.747,-180 320.747,-174 326.747,-174 326.747,-174 418.747,-174 418.747,-174 424.747,-174 430.747,-180 430.747,-186 430.747,-186 430.747,-198 430.747,-198 430.747,-204 424.747,-210 418.747,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"372.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Tendency &gt; &#45;0.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>7&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M356.355,-173.799C344.03,-160.865 327.021,-143.016 313.548,-128.878\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.912,-126.285 306.48,-121.461 310.845,-131.115 315.912,-126.285\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"352.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>7&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.996,-173.799C379.871,-162.163 383.729,-146.548 387.018,-133.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"390.503,-133.723 389.503,-123.175 383.707,-132.044 390.503,-133.723\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"398.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M410.247,-297C410.247,-297 335.247,-297 335.247,-297 329.247,-297 323.247,-291 323.247,-285 323.247,-285 323.247,-273 323.247,-273 323.247,-267 329.247,-261 335.247,-261 335.247,-261 410.247,-261 410.247,-261 416.247,-261 422.247,-267 422.247,-273 422.247,-273 422.247,-285 422.247,-285 422.247,-291 416.247,-297 410.247,-297\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"372.747\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ALTV &gt; 68.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;4 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>8&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M335.714,-260.799C306.846,-247.39 266.606,-228.7 235.686,-214.338\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.956,-211.069 226.412,-210.03 234.007,-217.417 236.956,-211.069\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"307.747\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;7 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>8&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.747,-260.799C372.747,-249.163 372.747,-233.548 372.747,-220.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"376.247,-220.175 372.747,-210.175 369.247,-220.175 376.247,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"386.247\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"403.747\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"403.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"508.747\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"508.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M545.247,-123C545.247,-123 472.247,-123 472.247,-123 466.247,-123 460.247,-117 460.247,-111 460.247,-111 460.247,-99 460.247,-99 460.247,-93 466.247,-87 472.247,-87 472.247,-87 545.247,-87 545.247,-87 551.247,-87 557.247,-93 557.247,-99 557.247,-99 557.247,-111 557.247,-111 557.247,-117 551.247,-123 545.247,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"508.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Width &gt; 72.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>11&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M487.499,-86.799C471.019,-73.4587 448.081,-54.8897 430.376,-40.5573\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.131,-37.4748 422.157,-33.9031 427.727,-42.9155 432.131,-37.4748\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"477.747\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>11&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M508.747,-86.799C508.747,-75.1626 508.747,-59.5479 508.747,-46.2368\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"512.247,-46.1754 508.747,-36.1754 505.247,-46.1755 512.247,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"522.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"623.747\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"623.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"725.747\" cy=\"-18\" rx=\"35.9954\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"725.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normal</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M674.247,-123C674.247,-123 587.247,-123 587.247,-123 581.247,-123 575.247,-117 575.247,-111 575.247,-111 575.247,-99 575.247,-99 575.247,-93 581.247,-87 587.247,-87 587.247,-87 674.247,-87 674.247,-87 680.247,-87 686.247,-93 686.247,-99 686.247,-99 686.247,-111 686.247,-111 686.247,-117 680.247,-123 674.247,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"630.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Variance &gt; 23.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;12 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>14&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M629.33,-86.799C628.372,-75.1626 627.086,-59.5479 625.99,-46.2368\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"629.47,-45.8544 625.161,-36.1754 622.494,-46.429 629.47,-45.8544\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"642.747\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;13 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>14&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M649.971,-86.799C664.653,-73.6626 685.002,-55.4561 700.916,-41.2174\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"703.661,-43.4573 708.78,-34.181 698.994,-38.2406 703.661,-43.4573\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"698.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>15</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M567.747,-210C567.747,-210 497.747,-210 497.747,-210 491.747,-210 485.747,-204 485.747,-198 485.747,-198 485.747,-186 485.747,-186 485.747,-180 491.747,-174 497.747,-174 497.747,-174 567.747,-174 567.747,-174 573.747,-174 579.747,-180 579.747,-186 579.747,-186 579.747,-198 579.747,-198 579.747,-204 573.747,-210 567.747,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"532.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Max &gt; 220.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;11 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>15&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.89,-173.799C524.572,-162.047 520.108,-146.238 516.326,-132.842\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"519.682,-131.848 513.596,-123.175 512.945,-133.75 519.682,-131.848\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"537.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>15&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M552.579,-173.799C567.19,-161.126 587.242,-143.734 603.379,-129.738\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"605.683,-132.372 610.945,-123.175 601.097,-127.084 605.683,-132.372\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"601.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>16</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"817.747\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"817.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>17</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"909.747\" cy=\"-18\" rx=\"35.9954\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"909.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normal</text>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>18</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M880.747,-123C880.747,-123 824.747,-123 824.747,-123 818.747,-123 812.747,-117 812.747,-111 812.747,-111 812.747,-99 812.747,-99 812.747,-93 818.747,-87 824.747,-87 824.747,-87 880.747,-87 880.747,-87 886.747,-87 892.747,-93 892.747,-99 892.747,-99 892.747,-111 892.747,-111 892.747,-117 886.747,-123 880.747,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"852.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Width &gt; 9?</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;16 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>18&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M845.664,-86.799C840.746,-74.8541 834.101,-58.7172 828.528,-45.1831\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"831.744,-43.7995 824.7,-35.8854 825.271,-46.4648 831.744,-43.7995\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"852.747\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;17 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>18&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M864.282,-86.799C872.554,-74.463 883.825,-57.6559 893.074,-43.8623\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"896.147,-45.5651 898.809,-35.3103 890.333,-41.6664 896.147,-45.5651\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"898.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>19</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"1011.75\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1011.75\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>20</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"1116.75\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1116.75\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>21</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M1054.75,-123C1054.75,-123 968.747,-123 968.747,-123 962.747,-123 956.747,-117 956.747,-111 956.747,-111 956.747,-99 956.747,-99 956.747,-93 962.747,-87 968.747,-87 968.747,-87 1054.75,-87 1054.75,-87 1060.75,-87 1066.75,-93 1066.75,-99 1066.75,-99 1066.75,-111 1066.75,-111 1066.75,-117 1060.75,-123 1054.75,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1011.75\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Median &gt; 128.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 21&#45;&gt;19 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>21&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1011.75,-86.799C1011.75,-75.1626 1011.75,-59.5479 1011.75,-46.2368\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1015.25,-46.1754 1011.75,-36.1754 1008.25,-46.1755 1015.25,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1026.75\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 21&#45;&gt;20 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>21&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1033,-86.799C1049.47,-73.4587 1072.41,-54.8897 1090.12,-40.5573\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1092.77,-42.9155 1098.34,-33.9031 1088.36,-37.4748 1092.77,-42.9155\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1084.25\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 22 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>22</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M877.747,-210C877.747,-210 827.747,-210 827.747,-210 821.747,-210 815.747,-204 815.747,-198 815.747,-198 815.747,-186 815.747,-186 815.747,-180 821.747,-174 827.747,-174 827.747,-174 877.747,-174 877.747,-174 883.747,-174 889.747,-180 889.747,-186 889.747,-186 889.747,-198 889.747,-198 889.747,-204 883.747,-210 877.747,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"852.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DP &gt; 1.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 22&#45;&gt;18 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>22&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M852.747,-173.799C852.747,-162.163 852.747,-146.548 852.747,-133.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"856.247,-133.175 852.747,-123.175 849.247,-133.175 856.247,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"867.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 22&#45;&gt;21 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>22&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M884.923,-173.799C909.68,-160.564 944.065,-142.182 970.787,-127.897\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"972.722,-130.831 979.89,-123.03 969.421,-124.658 972.722,-130.831\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"956.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 23 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>23</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M570.747,-297C570.747,-297 494.747,-297 494.747,-297 488.747,-297 482.747,-291 482.747,-285 482.747,-285 482.747,-273 482.747,-273 482.747,-267 488.747,-261 494.747,-261 494.747,-261 570.747,-261 570.747,-261 576.747,-261 582.747,-267 582.747,-273 582.747,-273 582.747,-285 582.747,-285 582.747,-291 576.747,-297 570.747,-297\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"532.747\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Mean &gt; 107.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 23&#45;&gt;15 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>23&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M532.747,-260.799C532.747,-249.163 532.747,-233.548 532.747,-220.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"536.247,-220.175 532.747,-210.175 529.247,-220.175 536.247,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"547.747\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 23&#45;&gt;22 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>23&#45;&gt;22</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M583.099,-264.625C644.161,-248.405 746.278,-221.281 805.776,-205.477\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"806.892,-208.802 815.658,-202.852 805.095,-202.036 806.892,-208.802\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"726.247\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 24 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>24</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M504.247,-384C504.247,-384 427.247,-384 427.247,-384 421.247,-384 415.247,-378 415.247,-372 415.247,-372 415.247,-360 415.247,-360 415.247,-354 421.247,-348 427.247,-348 427.247,-348 504.247,-348 504.247,-348 510.247,-348 516.247,-354 516.247,-360 516.247,-360 516.247,-372 516.247,-372 516.247,-378 510.247,-384 504.247,-384\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"465.747\" y=\"-362.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MSTV &gt; 0.45?</text>\r\n",
       "</g>\r\n",
       "<!-- 24&#45;&gt;8 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>24&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.927,-347.799C433.187,-335.241 414.377,-318.049 399.137,-304.12\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"401.281,-301.338 391.539,-297.175 396.559,-306.505 401.281,-301.338\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"440.747\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 24&#45;&gt;23 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>24&#45;&gt;23</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M479.305,-347.799C488.932,-335.587 502.012,-318.992 512.822,-305.278\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"515.767,-307.196 519.209,-297.175 510.269,-302.862 515.767,-307.196\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"517.247\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1f99eee74c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw the tree\n",
    "final_cls.draw_tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv(\"sberbank.csv\")\n",
    "\n",
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Separate the input and output columns.\n",
    "X = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
    "\n",
    "# Split into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11619    16.266516\n",
       "11300    14.667796\n",
       "20241    15.830414\n",
       "21320    15.943742\n",
       "27572    15.438787\n",
       "           ...    \n",
       "30410    13.815511\n",
       "10361    16.185754\n",
       "10373    15.775605\n",
       "27781    15.150512\n",
       "30253    15.919645\n",
       "Name: price_doc, Length: 13406, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16758, 8)\n",
      "      price_doc  year  full_sq  life_sq  floor  num_room  kitch_sq  full_all\n",
      "7672   10100000  2013       73     36.0   17.0       2.0      11.0    102828\n",
      "8056    2750000  2013       11     11.0    2.0       1.0      12.0     75377\n",
      "8135    9000000  2013       53     30.0   10.0       2.0       8.0     68630\n",
      "8144    4457400  2013       41     37.0   13.0       1.0       1.0      9553\n",
      "8153    7011550  2013       77     41.0    2.0       3.0      12.0      9553\n"
     ]
    }
   ],
   "source": [
    "print(alldata.shape)\n",
    "print(alldata.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for DummyRegressor:\n",
      "-0.38925247260237567\n"
     ]
    }
   ],
   "source": [
    "# Dummy regressor\n",
    "m1 = DummyRegressor()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for DummyRegressor:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for Linear Regression:\n",
      "-0.3013986588767236\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "m1 = LinearRegression()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Linear Regression:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for Lasso regressor:\n",
      "-0.3010470671748872\n"
     ]
    }
   ],
   "source": [
    "# Lasso regressor\n",
    "m1 = Lasso()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Lasso regressor:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for RandomForest regressor:\n",
      "-0.2830200215609616\n"
     ]
    }
   ],
   "source": [
    "# Random forest regressor\n",
    "m1 = RandomForestRegressor()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for RandomForest regressor:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for GradientBoosting Regressor:\n",
      "-0.2645738267201068\n"
     ]
    }
   ],
   "source": [
    "# GradientBoosting regressor\n",
    "m1 = GradientBoostingRegressor()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for GradientBoosting Regressor:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for MLP Regressor:\n",
      "-61390.260850944185\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor\n",
    "m1 = MLPRegressor()\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for MLP Regressor:\n",
    "{np.mean(cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting regressor performed the best on the default settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\"learning_rate\":[0.05,0.1,0.2],\n",
    "    \"max_depth\":[3,5,7],\n",
    "    \"n_estimators\":[100,200,300]}]\n",
    "\n",
    "regressor = GridSearchCV(estimator = GradientBoostingRegressor(), param_grid=params, scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "\n",
    "regressor_fit = regressor.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for GradientBoosting regressor: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best params for GradientBoosting regressor: {regressor_fit.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE on test data for our model: 0.2613252989407654\n"
     ]
    }
   ],
   "source": [
    "# Run the model on test data with the best parameters\n",
    "regressor = GradientBoostingRegressor(learning_rate = 0.2, max_depth = 3, n_estimators = 300)\n",
    "regressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "#Predict\n",
    "Yguess = regressor.predict(Xtest)\n",
    "print(f\"Final MSE on test data for our model: {mean_squared_error(Ytest, Yguess)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the Gradientboost regressor and received a MSE of approximately 0.26 on test data. We tuned the hyperparameter max_depth, learning rate and n_estimators. GradientBoosting performed the best both for the classification task and the regression task. \n",
    "\n",
    "### Task 4\n",
    "\n",
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for TreeRegressor\n",
    "class TreeRegressor(DecisionTree, RegressorMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='var_red'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'var_red':\n",
    "            self.criterion_function = variance_reduction\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Y\n",
    "        return np.mean(self.class_distribution)\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return np.var(self.class_distribution) < 0.1\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])        \n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "        values_var = np.var(Y)\n",
    "\n",
    "        low_ss = 0\n",
    "        low_sum = 0\n",
    "\n",
    "        high_ss = np.sum(np.array(Y)**2)\n",
    "        high_sum = np.sum(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "            \n",
    "            # Update\n",
    "            low_ss += (y_i**2)\n",
    "            low_sum += y_i\n",
    "\n",
    "            high_ss -= (y_i**2)\n",
    "            high_sum -= y_i\n",
    "\n",
    "            \n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i+1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            high_var = ((1/(n-i-1))*(high_ss))-((1/((n-i-1)**2))*(high_sum**2))\n",
    "            low_var = ((1/(i+1))*(low_ss))-((1/((i+1)**2))*(low_sum**2))\n",
    "\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(n, values_var, i+1, low_var, n-i-1, high_var)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(X_sorted[max_i] + X_sorted[max_i+1])\n",
    "        return max_score, feature, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance reduction function\n",
    "def variance_reduction(n, values_var, n_low, low_var, n_high, high_var):\n",
    "    return values_var-((n_low/n)*low_var)-((n_high/n)*high_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for depth=1: 0.009268460040512419\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"180pt\" height=\"131pt\"\r\n",
       " viewBox=\"0.00 0.00 180.34 131.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 127)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-127 176.343,-127 176.343,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"46.7958\" cy=\"-18\" rx=\"46.5926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"46.7958\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">&#45;0.006074</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"141.796\" cy=\"-18\" rx=\"30.5947\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"141.796\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.992</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M123.796,-123C123.796,-123 63.7958,-123 63.7958,-123 57.7958,-123 51.7958,-117 51.7958,-111 51.7958,-111 51.7958,-99 51.7958,-99 51.7958,-93 57.7958,-87 63.7958,-87 63.7958,-87 123.796,-87 123.796,-87 129.796,-87 135.796,-93 135.796,-99 135.796,-99 135.796,-111 135.796,-111 135.796,-117 129.796,-123 123.796,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"93.7958\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">F0 &gt; 1.038?</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84.2847,-86.799C77.6157,-74.7381 68.5836,-58.4034 61.056,-44.7896\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.0344,-42.943 56.1325,-35.8854 57.9085,-46.3303 64.0344,-42.943\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"88.7958\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M103.509,-86.799C110.409,-74.5805 119.786,-57.9757 127.533,-44.2572\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.716,-45.7388 132.585,-35.3103 124.62,-42.2968 130.716,-45.7388\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"133.296\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1f99f39ee80>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_some_data(n):\n",
    "    x = np.random.uniform(-5, 5, size=n)\n",
    "    Y = (x > 1) + 0.1*np.random.normal(size=n)\n",
    "    X = x.reshape(n, 1) # X needs to be a 2-dimensional matrix\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = make_some_data(100)\n",
    "cls = TreeRegressor(max_depth=1)\n",
    "cls.fit(X, Y)\n",
    "preds = cls.predict(X)\n",
    "print(f\"Mean squared error for depth=1: {mean_squared_error(preds, Y)}\")\n",
    "cls.draw_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for depth=5: 0.009276899357538443\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"166pt\" height=\"131pt\"\r\n",
       " viewBox=\"0.00 0.00 166.19 131.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 127)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-127 162.194,-127 162.194,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"39.6465\" cy=\"-18\" rx=\"39.7935\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"39.6465\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.01223</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"127.646\" cy=\"-18\" rx=\"30.5947\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"127.646\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.023</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M113.646,-123C113.646,-123 53.6465,-123 53.6465,-123 47.6465,-123 41.6465,-117 41.6465,-111 41.6465,-111 41.6465,-99 41.6465,-99 41.6465,-93 47.6465,-87 53.6465,-87 53.6465,-87 113.646,-87 113.646,-87 119.646,-87 125.646,-93 125.646,-99 125.646,-99 125.646,-111 125.646,-111 125.646,-117 119.646,-123 113.646,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"83.6465\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">F0 &gt; 1.055?</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.7424,-86.799C68.4584,-74.6593 59.9329,-58.1897 52.8584,-44.5231\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.9432,-42.8687 48.2378,-35.597 49.7267,-46.0867 55.9432,-42.8687\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"79.6465\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.5505,-86.799C98.8754,-74.5805 107.471,-57.9757 114.572,-44.2572\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"117.715,-45.8 119.204,-35.3103 111.498,-42.582 117.715,-45.8\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121.146\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1f99f39e820>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = make_some_data(100)\n",
    "cls = TreeRegressor(max_depth=5)\n",
    "cls.fit(X, Y)\n",
    "preds = cls.predict(X)\n",
    "print(f\"Mean squared error for depth=5: {mean_squared_error(preds, Y)}\")\n",
    "cls.draw_tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "To describe this data we would want a regression decision tree that can do a linear split. In our case, we would want a tree that checks if the X value is greater than 1 for the first node. \n",
    "\n",
    "We selected tree depth = 1 because we only need to check if X is greater than 1. However, the MSE for a tree with higher dept will be lower because the tree outputs a numerical value. If we have depth=1 we will only output two different values. For a tree with depth=2 this will instead be four values. \n",
    "\n",
    "When increasing the depth the plot becomes trickier to follow. However, we can see that it predicts the data slightly better. For generalization we believe it's better to only have depth=1.\n",
    "\n",
    "### Step 3\n",
    "\n",
    "We run CV for four different models with increasing depth. We choose the one with the best mean neg. MSE to run on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean negative MSE from 5-fold CV for Decision tree regressor depth=2:\n",
      "-0.3043367605680851\n",
      "Mean negative MSE from 5-fold CV for Decision tree regressor depth=5:\n",
      "-0.282804050256642\n",
      "Mean negative MSE from 5-fold CV for Decision tree regressor depth=7:\n",
      "-0.28269271168147114\n",
      "Mean negative MSE from 5-fold CV for Decision tree regressor depth=10:\n",
      "-0.30638166539002526\n"
     ]
    }
   ],
   "source": [
    "cls_2 = TreeRegressor(max_depth = 2)\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Decision tree regressor depth=2:\n",
    "{np.mean(cross_validate(cls_2, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")\n",
    "\n",
    "\n",
    "cls_5 = TreeRegressor(max_depth = 5)\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Decision tree regressor depth=5:\n",
    "{np.mean(cross_validate(cls_5, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")\n",
    "\n",
    "cls_7 = TreeRegressor(max_depth = 7)\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Decision tree regressor depth=7:\n",
    "{np.mean(cross_validate(cls_7, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")\n",
    "\n",
    "cls_10 = TreeRegressor(max_depth = 10)\n",
    "print(f\"\"\"Mean negative MSE from 5-fold CV for Decision tree regressor depth=10:\n",
    "{np.mean(cross_validate(cls_10, Xtrain, Ytrain, scoring='neg_mean_squared_error')[\"test_score\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on test data with depth=10: 0.2903456812456766\n"
     ]
    }
   ],
   "source": [
    "# Run best performing tree on test data\n",
    "cls = TreeRegressor(max_depth = 7)\n",
    "cls.fit(Xtrain, Ytrain)\n",
    "preds = cls.predict(Xtest)\n",
    "print(f\"Mean squared error on test data with depth=10: {mean_squared_error(Ytest, preds)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that neg. MSE was best for depth=7, but barely. Therefore we decided to use that model for our test data.\n",
    "We received a MSE of 0.29 on our test data.\n",
    "\n",
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot underfitting/overfitting\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "depth = np.arange(1,13)\n",
    "\n",
    "for i in range(0,12):\n",
    "    tree_regressor =TreeRegressor(max_depth=i+1)\n",
    "    tree_regressor.fit(Xtrain, Ytrain)\n",
    "    tree_regressor.draw_tree()\n",
    "\n",
    "    score = mean_squared_error(Ytrain, tree_regressor.predict(Xtrain))\n",
    "    train_scores.append(score)\n",
    "\n",
    "    score = mean_squared_error(Ytest, tree_regressor.predict(Xtest))\n",
    "    test_scores.append(score)\n",
    "    \n",
    "\n",
    "plt.plot(depth, train_scores, color='blue', label='Training data')\n",
    "plt.plot(depth, test_scores, color='red', label='Test data')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Depth VS Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the curve for test data is decreasing up until depth 8/9 when it instead starts to increase. The curve for the training data however is decreasing over all depths. This indicates that the model is starting to overfit to the training data around depth 8/9.\n",
    "\n",
    "If the curve for the test data decreased as well when the depth increased, the model would porbably not be overfit. But this is not the case now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05cd544f4d6121f76c83693c60af80a51020f4ceedd5e8b5e0ea7b59347d7a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are not linearly separable for the second classifier. The month of July has rain in Sydney but not Paris. The month of December has sun in Sydney but not Paris. A simple perceptron cannot handle the case of a XOR function. \n",
    "\n",
    "For the first classifier, we know that if we look at Gothenburg there will always be rain, so the classifier only need to figure out which month we're in to decided the weather for Paris. The inputs matters for the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(accuracy_score(Y1, guesses1))\n",
    "\n",
    "classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "#classifier2 = make_pipeline(DictVectorizer(), LinearSVC())\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(accuracy_score(Y2, guesses2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    self.w += y*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasos SVC implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of Pegasos SVC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=0, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Initialize counter\n",
    "        t = 0\n",
    "\n",
    "        # Pegasos algo:\n",
    "        for i in range(self.n_iter):\n",
    "            # Initialize the sum of loss values\n",
    "            loss_sum = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t += 1\n",
    "                eta = 1/(self.regularization*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If y*score is less than 1, update weight vector with gradient of the hinge loss\n",
    "                if y*score < 1:\n",
    "                    self.w = (1-eta*self.regularization)*self.w+eta*y*x\n",
    "                    # Update the sum of loss values\n",
    "                    loss_sum += 1-y*score\n",
    "                # Else, update weight vector with regularization\n",
    "                else:\n",
    "                    self.w *= (1-eta*self.regularization)\n",
    "\n",
    "            # Print current epoch and value of the objective function\n",
    "            print(f\"Epoch {i+1}, Value of obj.function: {(loss_sum/len(X))+self.regularization}\")\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of Logistic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=0, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Initialize counter\n",
    "        t = 0\n",
    "\n",
    "        # LR algo algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            # Initialize the sum of loss values\n",
    "            loss_sum = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t += 1\n",
    "                eta = 1/(self.regularization*t)\n",
    "\n",
    "                # Compute score for this instance\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # Gradient\n",
    "                gradient = 1/(1+np.exp(y*score))\n",
    "                # Update the weight vector\n",
    "                self.w = (1-eta*self.regularization)*self.w+eta*y*gradient*x\n",
    "\n",
    "                # Update the sum of loss values\n",
    "                loss_sum += -np.log(1-gradient)\n",
    "        \n",
    "            # Print current epoch and value of the objective function\n",
    "            print(f\"Epoch {i+1}, Value of obj.function: {(loss_sum/len(X))+self.regularization}\")\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus task 1\n",
    "\n",
    "a) BLAS implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import blas\n",
    "class blas_Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of Pegasos SVC with BLAS functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=0, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Initialize counter\n",
    "        t = 0\n",
    "\n",
    "        # Pegasos algo:\n",
    "        for i in range(self.n_iter):\n",
    "            # Initialize the sum of loss values\n",
    "            loss_sum = 0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t += 1\n",
    "                eta = 1/(self.regularization*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = blas.ddot(x, self.w)\n",
    "\n",
    "                # If y*score is less than 1, update weight vector with gradient of the hinge loss\n",
    "                if y*score < 1:\n",
    "                    # Scale weight vector\n",
    "                    y_a = blas.dscal((1-eta*self.regularization), self.w)\n",
    "                    # Update weight vector\n",
    "                    self.w = blas.daxpy(x, y_a, a = (eta*y))\n",
    "                    # Update the sum of loss values\n",
    "                    loss_sum += 1-y*score\n",
    "                # Else, update weight vector without gradient\n",
    "                else:\n",
    "                    self.w = blas.dscal((1-eta*self.regularization), self.w)\n",
    "\n",
    "            # Print current epoch and value of the objective function\n",
    "            print(f\"Epoch {i+1}, Value of obj.function: {(loss_sum/len(X))+self.regularization}\")\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b & c) Sparse vectors implementation with rescaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of Pegasos SVC using sparse vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=0, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the Pegasos SVC learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "        print(self.w.shape)\n",
    "        # Initialize counter\n",
    "        t = 0\n",
    "        # Pegasos algo:\n",
    "        for i in range(self.n_iter):\n",
    "            # Initialize the sum of loss values\n",
    "            loss_sum = 0\n",
    "\n",
    "            for x, y in XY:\n",
    "                t += 1\n",
    "                # Learning rate\n",
    "                eta = 1/(self.regularization*t)\n",
    "                # Compute the output score for this instance.\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # If y*score is less than 1, update weight vector with gradient of the hinge loss\n",
    "                if y*score < 1:\n",
    "                    self.w *= (1-eta*self.regularization)\n",
    "                    add_sparse_to_dense(x, self.w, (eta*y))\n",
    "                    # Update the sum of loss values\n",
    "                    loss_sum += 1-y*score\n",
    "                # Else, update weight vector without gradient\n",
    "                else:\n",
    "                    self.w *= (1-eta*self.regularization)\n",
    "\n",
    "            # Print current epoch and value of the objective function\n",
    "            print(f\"Epoch {i+1}, Value of obj.function: {(loss_sum/X.shape[0])+self.regularization}\")\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_Pegasos_SVC_scaling(LinearClassifier):\n",
    "    \"\"\"\n",
    "    Implementation of Pegasos SVC using sparse veectors and a rescaling operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization=0, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.regularization = regularization\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the Pegasos SVC learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        # Initialize counter\n",
    "        t = 0\n",
    "        # Pegasos algo:\n",
    "        for i in range(self.n_iter):\n",
    "            # Initialize the sum of loss values\n",
    "            loss_sum = 0\n",
    "            # Initialize scaling factor\n",
    "            a = 1\n",
    "\n",
    "            for x, y in XY:\n",
    "                t += 1\n",
    "                # Learning rate\n",
    "                eta = 1/(self.regularization*t)\n",
    "                # Compute the output score for this instance.\n",
    "                score = sparse_dense_dot(x, self.w)*a\n",
    "\n",
    "                # If y*score is less than 1, update weight vector with gradient of the hinge loss\n",
    "                if y*score < 1:\n",
    "                    a *= (1-eta*self.regularization)\n",
    "                    # Check for ZeroDivisionError\n",
    "                    if a != 0:\n",
    "                        add_sparse_to_dense(x, self.w, ((eta*y)/a))\n",
    "                    else:\n",
    "                        # If a was 0, set it to some small default value\n",
    "                        add_sparse_to_dense(x, self.w, eta*y/0.00001)\n",
    "                    # Update the sum of loss values\n",
    "                    loss_sum += 1-y*score\n",
    "                # Else, update weight vector without gradient\n",
    "                else:\n",
    "                    a *= (1-eta*self.regularization)\n",
    "            \n",
    "            # Postponed scaling operation   \n",
    "            self.w *= a\n",
    "\n",
    "            # Print current epoch and value of the objective function\n",
    "            print(f\"Epoch {i+1}, Value of obj.function: {(loss_sum/X.shape[0])+self.regularization}\")\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Value of obj.function: 1.1909944377665065\n",
      "Epoch 2, Value of obj.function: 0.37185947122254825\n",
      "Epoch 3, Value of obj.function: 0.34274596029431936\n",
      "Epoch 4, Value of obj.function: 0.32858046666425966\n",
      "Epoch 5, Value of obj.function: 0.32156240203756964\n",
      "Epoch 6, Value of obj.function: 0.3157702532831923\n",
      "Epoch 7, Value of obj.function: 0.3123038380929444\n",
      "Epoch 8, Value of obj.function: 0.3093708626908032\n",
      "Epoch 9, Value of obj.function: 0.30725373333243494\n",
      "Epoch 10, Value of obj.function: 0.3051811171838598\n",
      "Epoch 11, Value of obj.function: 0.3038702235797561\n",
      "Epoch 12, Value of obj.function: 0.30255867351836885\n",
      "Epoch 13, Value of obj.function: 0.3016966598536792\n",
      "Epoch 14, Value of obj.function: 0.30051354789804713\n",
      "Epoch 15, Value of obj.function: 0.2998919253892097\n",
      "Epoch 16, Value of obj.function: 0.2990332323534156\n",
      "Epoch 17, Value of obj.function: 0.29872793812690696\n",
      "Epoch 18, Value of obj.function: 0.2980287376510177\n",
      "Epoch 19, Value of obj.function: 0.29764080275412547\n",
      "Epoch 20, Value of obj.function: 0.297126992878879\n",
      "-----------\n",
      "Training time: 4.09 sec.\n",
      "Accuracy: 0.8317.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,1)),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "    #Perceptron()\n",
    "    \n",
    "    Pegasos_SVC(regularization=1/len(Ytrain), n_iter=20)\n",
    "    #LR(regularization=(1/len(Ytrain)), n_iter=20)\n",
    "\n",
    "    # Bonus\n",
    "    #blas_Pegasos_SVC(regularization=1/len(Ytrain), n_iter=50)\n",
    "    #sparse_Pegasos_SVC(regularization=1/len(Ytrain), n_iter=20)\n",
    "    #sparse_Pegasos_SVC_scaling(regularization=1/len(Ytrain), n_iter=20)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasos SVC and Logistic regression comparison\n",
    "\n",
    "Both implementations were run with the \"default\" settings, SelectKBest(1000), ngram_range=(1,1), lambda=1/n and we iterated 20 times through the training set during training.\n",
    "\n",
    "Pegasos SVC 0.8317 accuracy (20 iterations, 3.95s)\n",
    "LR 0.8326 accuracy (20 iterations, 7.87s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time comparisons\n",
    "\n",
    "### BLAS implementation for SVC vs regular:\n",
    "\n",
    "We ran the regular Pegasos implementation for 50 iteration and received an accuracy of approximately 83%. The training time was a total of 8.21 seconds. With the BLAS implementation we received exactly the same accuracy but the training time had decreased to 5.62 seconds. \n",
    "\n",
    "### Sparse vectors vs regular:\n",
    "\n",
    "For this we first removed the SelectKBest step from the pipeline and changed the ngram_range parameter of the TfidfVectorizer to (1,2). This significantly increases the number of features, from 1000 features to 480514 features. However, we did not manage to run the initial implementation of the Pegasos SVC with this many features. Therefore we changed the ngram_range parameter back to (1,1), \n",
    "\n",
    "With the sparse implementation the training time for 20 iterations was 47.07s and the accuracy was 0.8695.\n",
    "\n",
    "### Sparse vectors + speeding up the scaling operation:\n",
    "\n",
    "When we implemented the rescaling operation the training time for 20 iterations decreased significantly to 10.71s and the accuracy stayed around the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e70b1cb7551fd6ea98797562a3697049556a140f30ed21eba8d4e3d93ef6ee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
